{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu_is53zj_au",
        "outputId": "8ef12098-dbaa-4ea9-e987-5b4db66688db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.10/dist-packages (from newsapi-python) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (2.0.12)\n",
            "Installing collected packages: newsapi-python\n",
            "Successfully installed newsapi-python-0.2.7\n"
          ]
        }
      ],
      "source": [
        "!pip3 install newsapi-python"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QHAtrBCkm-W8"
      },
      "source": [
        "The \"newsapi-python\" package is a wrapper for the News API, which is a service that provides access to news articles from various sources around the world. By installing this package, developers can use Python to interact with the News API and retrieve news articles based on specific criteria such as keywords, sources, and time periods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wVRl59Z0lIgT"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from newsapi import NewsApiClient\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NHkAF2ubngzO"
      },
      "source": [
        "This code imports several Python packages:\n",
        "\n",
        "- tqdm: A package that provides progress bars for long-running loops or tasks.\n",
        "- pandas: A package for data manipulation and analysis.\n",
        "- numpy: A package for numerical computing in Python.\n",
        "- newsapi: A package for accessing news articles from various sources around the world.\n",
        "- nltk: The Natural Language Toolkit, a package for working with human language data in Python.\n",
        "- spacy: A package for natural language processing in Python.\n",
        "- sklearn: The Scikit-learn package, a machine learning library for Python.\n",
        "- matplotlib: A plotting library for Python.\n",
        "seaborn: A data visualization library based on Matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cHgPEmW0lOQc"
      },
      "outputs": [],
      "source": [
        "api_key = '946472f8ad574082a31c9d0fcaf1e89d'\n",
        "newsapi = NewsApiClient(api_key=api_key)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TrCKiZXgoA0U"
      },
      "source": [
        "The first line assigns a string value to the variable api_key. This value is an authentication key that is required to access the News API. It is a unique identifier that is associated with a specific user account and allows the user to make requests to the API.\n",
        "\n",
        "The second line creates an instance of the NewsApiClient class and assigns it to the variable newsapi. The NewsApiClient class is defined in the newsapi package and provides a set of methods that allow developers to interact with the News API.\n",
        "\n",
        "The api_key argument is passed to the NewsApiClient constructor to authenticate the client and enable it to make requests to the API on behalf of the user account associated with the provided API key. Once this instance of the NewsApiClient class is created, it can be used to call various methods that retrieve news articles from the API based on specific criteria such as keywords, sources, and time periods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SoddHCbYlSln"
      },
      "outputs": [],
      "source": [
        "def crawl_news(query):\n",
        "    all_results = []\n",
        "    for pag in tqdm(range(1, 6)):\n",
        "        pag_articles = newsapi.get_everything(q=query, sort_by='relevancy', page=pag)['articles']\n",
        "        if len(pag_articles) == 0:\n",
        "            break\n",
        "        all_results.extend(pag_articles)\n",
        "    return all_results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6Njsly3lpHO2"
      },
      "source": [
        "This is a Python function that uses the newsapi package to crawl news articles from various sources based on a search query.\n",
        "\n",
        "The function takes a single argument, query, which is a string representing the search term or topic of interest.\n",
        "\n",
        "The for loop iterates over 5 pages of news articles (i.e., range(1,6)). For each page, the newsapi.get_everything() method is called with the q argument set to the query parameter and the sort_by argument set to 'relevancy'. This retrieves news articles from the News API that match the search query and sorts them by relevance. The page argument is set to the current page number in the loop, which allows the function to retrieve news articles from multiple pages of search results.\n",
        "\n",
        "The retrieved news articles are stored in the pag_articles variable as a list of dictionaries, with each dictionary representing an article and containing various metadata such as the article's title, author, source, publication date, and content.\n",
        "\n",
        "If the length of pag_articles is zero, the loop breaks, as this indicates that there are no more pages of search results to retrieve.\n",
        "\n",
        "Finally, the list of all retrieved news articles across all pages is returned as the output of the function.\n",
        "\n",
        "The function uses the tqdm package to display a progress bar during the loop, indicating the percentage of pages that have been crawled so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98ADci2alUra",
        "outputId": "bd0e8fe8-5560-4c4a-9102-c73a71961bad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00,  5.61it/s]\n"
          ]
        }
      ],
      "source": [
        "tesla_news = crawl_news('tesla')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N9c_zbyRlXuR"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('BBC news dataset.csv', usecols=range(1, 3))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5eRK00U8prLo"
      },
      "source": [
        "This line of code reads a CSV file called 'BBC news dataset.csv' using the pandas package, and creates a DataFrame object named df that contains the data from the CSV file.\n",
        "\n",
        "The usecols parameter is used to select only specific columns from the CSV file. In this case, the range(1, 3) argument selects columns 1 and 2, which correspond to the 'description' and 'tags' columns of the BBC news dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "moRaM-NMlaIs"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates('description', inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FEHXK08FqaFP"
      },
      "source": [
        "This line of code drops any duplicate rows in the DataFrame object df based on the values in the 'description' column, and modifies df in place by setting the inplace parameter to True.\n",
        "\n",
        "The drop_duplicates() method is a built-in method of the pandas package that removes duplicate rows from a DataFrame. The 'description' column is used as the key to identify duplicates. If there are any rows with identical values in the 'description' column, only the first occurrence of such a row is kept and all subsequent duplicates are removed.\n",
        "\n",
        "By setting inplace=True, the drop_duplicates() method modifies the DataFrame object df directly, without creating a new object. This means that any subsequent operations on df will reflect the changes made by this method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "06AWCS4BlcgL"
      },
      "outputs": [],
      "source": [
        "descriptions = df['description'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QVT3BCQ_le0o"
      },
      "outputs": [],
      "source": [
        "def pre_process_text(text):\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    # convert to lower case\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "    # remove punctuation from each word\n",
        "\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "    # filter out stop words\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    return words"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uYh4HTgJrMGJ"
      },
      "source": [
        "This is a function called pre_process_text that takes a string of text as its input, performs several text preprocessing steps, and returns a list of preprocessed words.\n",
        "\n",
        "The text preprocessing steps are as follows:\n",
        "\n",
        "1. Tokenization: The input text is split into individual words or tokens using the word_tokenize() function from the nltk package.\n",
        "2. Lowercasing: All words are converted to lowercase using a list comprehension.\n",
        "3. Removing punctuation: Punctuation marks are removed from each word using the string.punctuation string and the str.translate() method.\n",
        "4. Removing non-alphabetic characters: Words that contain non-alphabetic characters (such as numbers or symbols) are removed using a list comprehension and the str.isalpha() method.\n",
        "5. Removing stop words: Common stop words (such as \"the\", \"and\", \"a\") are removed using the stopwords.words() method from the nltk package and another list comprehension.\n",
        "\n",
        "The preprocessed words are returned as a list.\n",
        "\n",
        "This function can be used to preprocess the descriptions of news articles in the BBC news dataset before clustering or visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFLezKNOlwa6",
        "outputId": "b81a5550-75ff-4268-ae87-54142bbbd935"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5hksBGTWrqVz"
      },
      "source": [
        "The punkt and stopwords datasets are required for tokenization and stopword removal, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EeEw2H_lhhG",
        "outputId": "fae18b70-cf73-4b37-e8d5-1232b1107250"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2128/2128 [00:03<00:00, 639.06it/s]\n"
          ]
        }
      ],
      "source": [
        "processed_descriptions = []\n",
        "for description in tqdm(descriptions):\n",
        "    processed_descriptions.append(' '.join(pre_process_text(description)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EgvgN8N0sfoL"
      },
      "source": [
        "This is a loop that iterates over each description in the descriptions numpy array, applies the pre_process_text() function to each description, and appends the preprocessed description to a new list called processed_descriptions.\n",
        "\n",
        "The tqdm() function is used to display a progress bar that indicates how far along the loop is in processing the descriptions.\n",
        "\n",
        "Inside the loop, the pre_process_text() function is applied to each description. The resulting list of preprocessed words is joined into a single string using the join() method with a space separator. The joined string is then appended to the processed_descriptions list.\n",
        "\n",
        "After the loop is finished, the processed_descriptions list contains all of the preprocessed descriptions of the news articles in the BBC news dataset. Each preprocessed description is a single string containing lowercase words with no punctuation, non-alphabetic characters, or stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bdf6XJTl5jY",
        "outputId": "7dbb52e0-572b-4511-8073-42c02fc09244"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2128/2128 [01:10<00:00, 29.98it/s]\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "sent_vecs = {}\n",
        "docs = []\n",
        "\n",
        "for index, description in enumerate(tqdm(processed_descriptions)):\n",
        "    doc = nlp(description)\n",
        "    docs.append(doc)\n",
        "    sent_vecs[index] = doc.vector"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fvBCSfIEtdfi"
      },
      "source": [
        "This is a block of code that uses the spacy package to compute the vector representation of each preprocessed description in the processed_descriptions list.\n",
        "\n",
        "The first line loads a pre-trained model called en_core_web_sm from the spacy package. This model includes word vectors and linguistic annotations for the English language.\n",
        "\n",
        "The sent_vecs dictionary is initialized to an empty dictionary, and the docs list is initialized to an empty list.\n",
        "\n",
        "The code then loops through each preprocessed description in processed_descriptions. For each description, spacy is used to create a document object doc containing linguistic annotations and vectors for each word in the description.\n",
        "\n",
        "The document object doc is added to the docs list. The vector representation of the entire document is extracted from doc using the doc.vector attribute, and is stored in the sent_vecs dictionary with the index of the description as the key.\n",
        "\n",
        "After the loop is finished, the sent_vecs dictionary contains a mapping of each index to the vector representation of the corresponding preprocessed description in processed_descriptions. The docs list contains a spacy document object for each preprocessed description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Bv5dUfZVl9eI"
      },
      "outputs": [],
      "source": [
        "vectors = list(sent_vecs.values())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2opY4I5ot0Of"
      },
      "source": [
        "This line of code extracts the vector representations of the preprocessed descriptions from the sent_vecs dictionary and converts them to a list.\n",
        "\n",
        "The sent_vecs dictionary maps the index of each preprocessed description to its corresponding vector representation. The values() method of a dictionary returns a list of all the values in the dictionary, in this case a list of all the vector representations of the preprocessed descriptions.\n",
        "\n",
        "The list() function is used to convert the values() view object to a list. The resulting vectors list contains the vector representations of all the preprocessed descriptions in the BBC news dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PlGAW7U6l_IQ"
      },
      "outputs": [],
      "source": [
        "vectors = np.array(vectors)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
